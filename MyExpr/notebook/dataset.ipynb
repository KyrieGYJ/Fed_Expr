{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Fed_Expr/MyExpr/notebook\n",
      "['/home/ubuntu/Fed_Expr/MyExpr/notebook', '/home/ubuntu/miniconda/envs/fedml/lib/python37.zip', '/home/ubuntu/miniconda/envs/fedml/lib/python3.7', '/home/ubuntu/miniconda/envs/fedml/lib/python3.7/lib-dynload', '', '/home/ubuntu/miniconda/envs/fedml/lib/python3.7/site-packages', '/home/ubuntu/miniconda/envs/fedml/lib/python3.7/site-packages/IPython/extensions', '/home/ubuntu/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# 基本信息\n",
    "print(os.getcwd())\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "class DataLoader(object):\n",
    "    def __init__(self, data_name, data_path, client_list, sample_num_in_total, beta):\n",
    "        # SUSY, Room Occupancy;\n",
    "        self.data_name = data_name\n",
    "        self.data_path = data_path\n",
    "        self.client_list = client_list\n",
    "        self.sample_num_in_total = sample_num_in_total\n",
    "        self.beta = beta\n",
    "        self.streaming_full_dataset_X = []\n",
    "        self.streaming_full_dataset_Y = []\n",
    "        self.StreamingDataDict = {}\n",
    "\n",
    "    \"\"\"\n",
    "        return streaming_data\n",
    "            key: client_id\n",
    "            value: [sample1, sample2, ..., sampleN]\n",
    "                    sample: {\"x\": [1,2,3,4,5,...,M]; \"y\":0}\n",
    "    \"\"\"\n",
    "\n",
    "    def load_datastream(self):\n",
    "        # 1 put sample_num_in_total data in streaming_full_dataset_X and streaming_full_dataset_Y\n",
    "        self.preprocessing()\n",
    "        # print(self.streaming_full_dataset_X)\n",
    "        # print(self.streaming_full_dataset_Y)\n",
    "        \n",
    "        # 2 Kmeans cluster and assaign each client a cluster of data only sample_num_in_total*beta data are used\n",
    "        self.load_adversarial_data()\n",
    "        \n",
    "        # 3 （1）Kmeans聚类可能不均衡，先砍掉超出的，随机补上\n",
    "        #   （2）把剩余的 sample_num_in_total*（1 - beta）个数据补入每个类中\n",
    "        self.load_stochastic_data()\n",
    "        # for value in self.StreamingDataDict.values():\n",
    "        #     random.shuffle(value)\n",
    "\n",
    "        # for client_index in self.client_list:\n",
    "        #     length = len(self.StreamingDataDict[client_index])\n",
    "        #     logging.info(\"len of index %d = %d\" % (client_index, length))\n",
    "        return self.StreamingDataDict\n",
    "\n",
    "    # beta (clustering, GMM)\n",
    "    def load_adversarial_data(self):\n",
    "        streaming_data = self.read_csv_file_for_cluster(self.beta)\n",
    "        return streaming_data\n",
    "\n",
    "    def load_stochastic_data(self):\n",
    "        streaming_data = self.read_csv_file(self.beta)\n",
    "        return streaming_data\n",
    "\n",
    "    def read_csv_file(self, percent):\n",
    "        # print(\"start from:\")\n",
    "        iteration_number = int(self.sample_num_in_total / len(self.client_list))\n",
    "        index_start = int(percent * self.sample_num_in_total)\n",
    "        stochastic_data_x = []\n",
    "        stochastic_data_y = []\n",
    "        # 将 streaming_full_dataset 中大于 percent * self.sample_num_in_total 的部分放入 stochastic_data\n",
    "        for i_x, dp_x in enumerate(self.streaming_full_dataset_X):\n",
    "            if i_x >= index_start:\n",
    "                stochastic_data_x.append(dp_x)\n",
    "        for i_y, dp_y in enumerate(self.streaming_full_dataset_Y):\n",
    "            if i_y >= index_start:\n",
    "                stochastic_data_y.append(dp_y)\n",
    "        # 所有client中，取自身数据量大于均值的，surplus 放入 stochastic_data，并在它们的本地数据中去掉这部分数据\n",
    "        for c_index in self.client_list:\n",
    "            if len(self.StreamingDataDict[self.client_list[c_index]]) > iteration_number:\n",
    "                for i, data_point in enumerate(self.StreamingDataDict[self.client_list[c_index]]):\n",
    "                    if i >= iteration_number:\n",
    "                        stochastic_data_x.append(data_point['x'])\n",
    "                        stochastic_data_y.append(data_point['y'])\n",
    "                self.StreamingDataDict[self.client_list[c_index]] = self.StreamingDataDict[self.client_list[c_index]][0:iteration_number]\n",
    "\n",
    "        # print(\"***\")\n",
    "        # for c_index in self.client_list:\n",
    "        #     print(len(self.StreamingDataDict[self.client_list[c_index]]))\n",
    "        # print(\"***\")\n",
    "        client_index = 0\n",
    "        full_count = 0\n",
    "        # print(\"iteration_number = \" + str(iteration_number))\n",
    "        # print(\"len stochastic_data_x = \" + str(len(stochastic_data_x)))\n",
    "        for i in range(len(stochastic_data_x)):\n",
    "            while len(self.StreamingDataDict[self.client_list[client_index]]) == iteration_number:\n",
    "                client_index += 1\n",
    "                full_count += 1\n",
    "            sample = {}\n",
    "            sample[\"x\"] = stochastic_data_x[i]\n",
    "            sample[\"y\"] = stochastic_data_y[i]\n",
    "            self.StreamingDataDict[self.client_list[client_index]].append(sample)\n",
    "            \n",
    "            # 估摸着是为了防止越界，写得太臭了\n",
    "            if len(self.StreamingDataDict[self.client_list[client_index]]) == iteration_number and \\\n",
    "                    full_count == len(self.client_list) - 1:\n",
    "                full_count += 1\n",
    "            if full_count == len(self.client_list):\n",
    "                # print(\"stop at index = \" + str(i))\n",
    "                break\n",
    "        return self.StreamingDataDict\n",
    "\n",
    "    def read_csv_file_for_cluster(self, percent):\n",
    "        data = []\n",
    "        label = []\n",
    "        for client_id in self.client_list:\n",
    "            self.StreamingDataDict[client_id] = []\n",
    "        if percent == 0:\n",
    "            return self.StreamingDataDict\n",
    "\n",
    "        for i, row in enumerate(self.streaming_full_dataset_X):\n",
    "            if i >= (self.sample_num_in_total * percent):\n",
    "                break\n",
    "            data.append(self.streaming_full_dataset_X[i])\n",
    "            label.append(self.streaming_full_dataset_Y[i])\n",
    "        \n",
    "        # self.sample_num_in_total * percent条数据\n",
    "        # print(len(data))\n",
    "        # print(len(label))\n",
    "        \n",
    "        # print(\"Clustering Started\")\n",
    "        clusters = self.kMeans(data)\n",
    "        # print(\"Clustering Finished\")\n",
    "        \n",
    "        # 将data分类，cluster[i]表示data[i]的分类\n",
    "        # print(clusters)\n",
    "        # print(type(clusters))\n",
    "\n",
    "        for i, cluster in enumerate(clusters):\n",
    "            sample = {}\n",
    "            sample[\"y\"] = label[i]\n",
    "            sample[\"x\"] = data[i]\n",
    "            \n",
    "            # 将cluster分类放入对应的客户数据集中\n",
    "            self.StreamingDataDict[self.client_list[cluster]].append(sample)\n",
    "        # print(\"Arrange Clustered Data has Finished\")\n",
    "\n",
    "        # for id in self.client_list:\n",
    "            # print(\"after clustering:\")\n",
    "            # print(len(self.StreamingDataDict[self.client_list[id]]))\n",
    "        return self.StreamingDataDict\n",
    "\n",
    "    def kMeans(self, X):\n",
    "        kmeans = KMeans(n_clusters=len(self.client_list))\n",
    "        kmeans.fit(X)\n",
    "        return kmeans.labels_\n",
    "\n",
    "    def preprocessing(self):\n",
    "        # print(\"sample_num_in_total = \" + str(self.sample_num_in_total))\n",
    "        data = []\n",
    "        with open(self.data_path) as csvfile:\n",
    "            readCSV = csv.reader(csvfile, delimiter=\",\")\n",
    "            for i, row in enumerate(readCSV):\n",
    "                if i < self.sample_num_in_total:\n",
    "                    if self.data_name == \"SUSY\":\n",
    "                        data.append(np.asarray(row[1:], dtype=np.float32))\n",
    "                        self.streaming_full_dataset_Y.append(int(row[0].split('.')[0]))\n",
    "                    elif self.data_name == \"RO\":\n",
    "                        data.append(np.asarray(row[2:-1], dtype=np.float32))\n",
    "                        self.streaming_full_dataset_Y.append(int(row[-1].split('.')[0]))\n",
    "                else:\n",
    "                    break\n",
    "        # min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        # self.streaming_full_dataset_X = min_max_scaler.fit_transform(data)\n",
    "        self.streaming_full_dataset_X = data\n",
    "        # print(\"############\")\n",
    "        # print(len(self.streaming_full_dataset_X))\n",
    "        # print(self.streaming_full_dataset_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 3 3 1 4 1 4 3 0 1 0 1 1 3 1 4 3 4 0 1 0 3 0 2 0 1 0 3 0 1 0 2 4 4 3 1\n",
      " 3 4 3]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader(\"SUSY\", \"../../FedML/data/UCI/SUSY/SUSY.csv\", client_id_list, 5 * 10, 0.8)\n",
    "streaming_data = data_loader.load_datastream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 分析一下SUSY数据\n",
    "client_id_list = [i for i in range(5)]\n",
    "\n",
    "with open(\"../../FedML/data/UCI/SUSY/SUSY.csv\") as csvfile:\n",
    "    readCSV = csv.reader(csvfile, delimiter=\",\")\n",
    "    # 查看前10条\n",
    "    show = 10\n",
    "    total = 0\n",
    "    for row in readCSV:\n",
    "        # total += 1\n",
    "        # row[0]是类别（二分类，0/1），row[1:]为属性，总共18个属性\n",
    "        if show > 0:\n",
    "            print(len(row))\n",
    "            print(len(np.asarray(row[1:], dtype=np.float32)))\n",
    "            # 原类别是list\n",
    "            print(type(row))\n",
    "            # 改为numpy.ndarray\n",
    "            # print(type(np.asarray(row[1:], dtype=np.float32)))\n",
    "            print(row)\n",
    "            print(np.asarray(row[1:], dtype=np.float32))\n",
    "            break\n",
    "        show -= 1\n",
    "# len(SUSY) = 5,000,000\n",
    "# The last 500,000 examples are used as a test set.n about your data set.\n",
    "# print(\"total=\", total)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data/cifar-10-python.tar.gz\n",
      "Extracting ../data/cifar-10-python.tar.gz to ../data/\n",
      "===============================start==================================\n",
      "================================end===================================\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/170498071 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fbe8f0845d1341378316cc9fddd900eb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 9/782 [00:00<01:09, 11.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# 简单观察一下dataloader对于batchsize的设计\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "##transform\n",
    "def trainsform(mean,std):\n",
    "    transform_train = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomCrop(32,padding=4,padding_mode='reflect'),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean,std)\n",
    "        ])\n",
    "    transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean,std)\n",
    "        ])\n",
    "\n",
    "    return transform_train,transform_test\n",
    "\n",
    "dir_path = \"../data/\"\n",
    "batch_size = 64\n",
    "# ?\n",
    "num_workers = 2\n",
    "# CIFAR10\n",
    "cifar10_mean = (0.4914, 0.4822, 0.4465)\n",
    "cifar10_std = (0.2471, 0.2435, 0.2616)\n",
    "transform_train,transform_test=trainsform(cifar10_mean,cifar10_std)\n",
    "train_loader = DataLoader(\n",
    "    datasets.CIFAR10(root=dir_path, train=True, transform=transform_train,download=True),\n",
    "    batch_size = batch_size, shuffle = True, num_workers=num_workers\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    datasets.CIFAR10(root=dir_path, train=False, transform=transform_test),\n",
    "    batch_size = batch_size, shuffle = False, num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_epoch = 10\n",
    "print(\"===============================start==================================\")\n",
    "for batch_idx, (image, label) in enumerate(tqdm(train_loader)):\n",
    "    test_epoch -= 1\n",
    "    # print(batch_idx, (image, label))\n",
    "\n",
    "    if test_epoch == 0:\n",
    "        break\n",
    "print(\"================================end===================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}