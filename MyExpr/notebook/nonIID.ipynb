{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/guest/Fed_Expr', '/home/guest/Fed_Expr/FedML', '/home/guest/Fed_Expr/MyExpr', '/home/guest/Fed_Expr/MyExpr/notebook', '/home/guest/miniconda/envs/fedml/lib/python37.zip', '/home/guest/miniconda/envs/fedml/lib/python3.7', '/home/guest/miniconda/envs/fedml/lib/python3.7/lib-dynload', '', '/home/guest/miniconda/envs/fedml/lib/python3.7/site-packages', '/home/guest/miniconda/envs/fedml/lib/python3.7/site-packages/IPython/extensions', '/home/guest/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "# 添加环境\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"../../MyExpr\")))\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"../../FedML\")))\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"../../\")))\n",
    "\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "shard_per_user = 2\n",
    "num_users = 100\n",
    "num_classes = 10\n",
    "shard_per_class = int(shard_per_user * num_users / num_classes)\n",
    "\n",
    "# 随机给每个client分配 num_classes*shard_per_class 个 shard\n",
    "rand_set_all = list(range(num_classes)) * shard_per_class\n",
    "np.random.shuffle(rand_set_all)\n",
    "rand_set_all = np.array(rand_set_all).reshape((num_users, -1))\n",
    "# print(rand_set_all)\n",
    "\n",
    "# for _ in range(10):\n",
    "#     print(np.random.choice(10, replace=False))\n",
    "\n",
    "# d = {1:1, 2:3, 3:4}\n",
    "# print(d)\n",
    "# d.pop(1)\n",
    "# print(d)\n",
    "# for key, value in d.items():\n",
    "#     print(key, value)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Federated Learning')\n",
    "parser.add_argument('--data_seed', default=0, type=int, help=\"Random seed for initializing data\")\n",
    "args = parser.parse_known_args()[0]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def client_noniid(dataset, num_users, shard_per_user, rand_set_all=[], seed=args.data_seed):\n",
    "    \"\"\"\n",
    "    Sample non-IID client data from dataset in pathological manner - from LG-FedAvg implementation\n",
    "    :param dataset:\n",
    "    :param num_users:\n",
    "    :return: (dictionary, where keys = client_id / index, and values are dataset indices), rand_set_all (all classes)\n",
    "\n",
    "    shard_per_user should be a factor of the dataset size\n",
    "    \"\"\"\n",
    "    dict_users = {i: np.array([], dtype='int64') for i in range(num_users)}\n",
    "\n",
    "    idxs_dict = {}\n",
    "\n",
    "    # 将dataset中的数据放入字典\n",
    "    for i in range(len(dataset)):\n",
    "        label = torch.tensor(dataset.targets[i]).item()\n",
    "        if label not in idxs_dict.keys():\n",
    "            idxs_dict[label] = []\n",
    "        idxs_dict[label].append(i)\n",
    "\n",
    "    # 统计标签种类\n",
    "    num_classes = len(np.unique(dataset.targets))\n",
    "    # 计算每个类要分出多少个shard\n",
    "    shard_per_class = int(shard_per_user * num_users / num_classes)\n",
    "    \n",
    "    # 生成 label -> shards的字典\n",
    "    for label in idxs_dict.keys():\n",
    "        x = idxs_dict[label]\n",
    "        # 计算分shard后多余的数据量\n",
    "        num_leftover = len(x) % shard_per_class\n",
    "        # 记录余数\n",
    "        leftover = x[-num_leftover:] if num_leftover > 0 else []\n",
    "        # 裁剪多余数据\n",
    "        x = np.array(x[:-num_leftover]) if num_leftover > 0 else np.array(x)\n",
    "        # reshape成[shard_num, shard_size]\n",
    "        x = x.reshape((shard_per_class, -1))\n",
    "        x = list(x)\n",
    "\n",
    "        # 多余数据有限填充到x[i]\n",
    "        for i, idx in enumerate(leftover):\n",
    "            x[i] = np.concatenate([x[i], [idx]])\n",
    "        # idxs_dict: label -> [shard_num, shard_zie]\n",
    "        idxs_dict[label] = x\n",
    "\n",
    "    # 总共num_classes*shard_per_class个shard， 随机给每个client分配 \n",
    "    np.random.seed(seed)\n",
    "    if len(rand_set_all) == 0:\n",
    "        rand_set_all = list(range(num_classes)) * shard_per_class\n",
    "        np.random.shuffle(rand_set_all)\n",
    "        rand_set_all = np.array(rand_set_all).reshape((num_users, -1))\n",
    "\n",
    "    # 按照上述shard划分，为每个user分配对应的shard\n",
    "    # Divide and assign\n",
    "    np.random.seed(seed)\n",
    "    for i in range(num_users):\n",
    "        rand_set_label = rand_set_all[i]\n",
    "        rand_set = []\n",
    "        for label in rand_set_label:\n",
    "            # replace是指抽取不放回，但是这里并没有指定size，每次都独立取出一个idx。\n",
    "            idx = np.random.choice(len(idxs_dict[label]), replace=False)\n",
    "            # 弹出idx，防止重复取\n",
    "            rand_set.append(idxs_dict[label].pop(idx))\n",
    "        # print(rand_set)\n",
    "        # 将rand_set拼接成一条数据，放入dic_user[i]中\n",
    "        dict_users[i] = np.concatenate(rand_set)\n",
    "\n",
    "\n",
    "    test = []\n",
    "    for key, value in dict_users.items():\n",
    "        x = np.unique(torch.tensor(dataset.targets)[value])\n",
    "        assert(len(x)) <= shard_per_user\n",
    "        test.append(value)\n",
    "    test = np.concatenate(test)\n",
    "    assert(len(test) == len(dataset))\n",
    "    assert(len(set(list(test))) == len(dataset))\n",
    "\n",
    "    return dict_users, rand_set_all"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def compute_emd(targets_1, targets_2):\n",
    "    \"\"\"Calculates Earth Mover's Distance between two array-like objects (dataset labels)\"\"\"\n",
    "    total_targets = []\n",
    "    total_targets.extend(list(np.unique(targets_1)))\n",
    "    total_targets.extend(list(np.unique(targets_2)))\n",
    "\n",
    "    emd = 0\n",
    "\n",
    "    counts_1 = Counter(targets_1)\n",
    "    counts_2 = Counter(targets_2)\n",
    "\n",
    "    size_1 = len(targets_1)\n",
    "    size_2 = len(targets_2)\n",
    "\n",
    "    for t in counts_1:\n",
    "        count_2 = counts_2[t] if t in counts_2 else 0\n",
    "        emd += np.abs((counts_1[t] / size_1) - (count_2 / size_2))\n",
    "\n",
    "    for t in counts_2:\n",
    "        count_1 = counts_1[t] if t in counts_1 else 0\n",
    "        emd += np.abs((counts_2[t] / size_2) - (count_1 / size_1))\n",
    "\n",
    "    return emd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.datasets import utils, MNIST, CIFAR10\n",
    "\n",
    "tra_trans = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "val_trans = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "# len(trainset) = 50000 len(testset) = 10000\n",
    "trainset = CIFAR10(root=\"./../data\", train=True, download=True, transform=tra_trans)\n",
    "testset = CIFAR10(root=\"./../data\", train=False, download=True, transform=val_trans)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "d_u, rsa = client_noniid(trainset, 100, 5, seed=0)\n",
    "print(len(d_u))\n",
    "print(len(d_u[0]))\n",
    "print(len(np.unique(d_u[0])))\n",
    "\n",
    "# for key, value in d_u.items():\n",
    "#     # 抽出当前client包含的class\n",
    "#     # 有可能会给client放入同类\n",
    "#     x = np.unique(torch.tensor(trainset.targets)[value])\n",
    "#     # print(torch.tensor(trainset.targets)[value])\n",
    "#     print(\"x=\", x)\n",
    "#     # print(\"value=\", value)\n",
    "#     # break\n",
    "#     assert(len(x)) <= shard_per_user\n",
    "\n",
    "emd = []\n",
    "for ix in d_u:\n",
    "    client_targets = [trainset.targets[x] for x in d_u[ix]]\n",
    "    emd.append(compute_emd(client_targets, trainset.targets))\n",
    "print(emd)\n",
    "average_emd = np.mean([compute_emd([trainset.targets[x] for x in d_u[ix]], trainset.targets) for ix in d_u], axis=0)\n",
    "# average_emd = np.mean(emd)\n",
    "print(average_emd)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dic = {1:['a','b'], 3:['q','s'], 4:['g','u'], 5:['n','v']}\n",
    "for i in dic:\n",
    "    print(i)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Federated Learning')\n",
    "parser.add_argument('--data_seed', default=0, type=int, help=\"Random seed for initializing data\")\n",
    "args = parser.parse_known_args()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def client_noniid(dataset, num_users, shard_per_user, rand_set_all=[], seed=args.data_seed):\n",
    "    \"\"\"\n",
    "    Sample non-IID client data from dataset in pathological manner - from LG-FedAvg implementation\n",
    "    :param dataset:\n",
    "    :param num_users:\n",
    "    :return: (dictionary, where keys = client_id / index, and values are dataset indices), rand_set_all (all classes)\n",
    "\n",
    "    shard_per_user should be a factor of the dataset size\n",
    "    \"\"\"\n",
    "    dict_users = {i: np.array([], dtype='int64') for i in range(num_users)}\n",
    "\n",
    "    idxs_dict = {}\n",
    "\n",
    "    # 将dataset中的数据放入字典\n",
    "    for i in range(len(dataset)):\n",
    "        label = torch.tensor(dataset.targets[i]).item()\n",
    "        if label not in idxs_dict.keys():\n",
    "            idxs_dict[label] = []\n",
    "        idxs_dict[label].append(i)\n",
    "\n",
    "    # 统计标签种类\n",
    "    num_classes = len(np.unique(dataset.targets))\n",
    "    # 计算每个类要分出多少个shard\n",
    "    shard_per_class = int(shard_per_user * num_users / num_classes)\n",
    "    \n",
    "    # 生成 label -> shards的字典\n",
    "    for label in idxs_dict.keys():\n",
    "        x = idxs_dict[label]\n",
    "        # 计算分shard后多余的数据量\n",
    "        num_leftover = len(x) % shard_per_class\n",
    "        # 记录余数\n",
    "        leftover = x[-num_leftover:] if num_leftover > 0 else []\n",
    "        # 裁剪多余数据\n",
    "        x = np.array(x[:-num_leftover]) if num_leftover > 0 else np.array(x)\n",
    "        # reshape成[shard_num, shard_size]\n",
    "        x = x.reshape((shard_per_class, -1))\n",
    "        x = list(x)\n",
    "\n",
    "        # 多余数据有限填充到x[i]\n",
    "        for i, idx in enumerate(leftover):\n",
    "            x[i] = np.concatenate([x[i], [idx]])\n",
    "        # idxs_dict: label -> [shard_num, shard_zie]\n",
    "        idxs_dict[label] = x\n",
    "\n",
    "    # 总共num_classes*shard_per_class个shard， 随机给每个client分配 \n",
    "    np.random.seed(seed)\n",
    "    if len(rand_set_all) == 0:\n",
    "        rand_set_all = list(range(num_classes)) * shard_per_class\n",
    "        np.random.shuffle(rand_set_all)\n",
    "        rand_set_all = np.array(rand_set_all).reshape((num_users, -1))\n",
    "\n",
    "    # 按照上述shard划分，为每个user分配对应的shard\n",
    "    # Divide and assign\n",
    "    np.random.seed(seed)\n",
    "    for i in range(num_users):\n",
    "        rand_set_label = rand_set_all[i]\n",
    "        rand_set = []\n",
    "        for label in rand_set_label:\n",
    "            # replace是指抽取不放回，但是这里并没有指定size，每次都独立取出一个idx。\n",
    "            idx = np.random.choice(len(idxs_dict[label]), replace=False)\n",
    "            # 弹出idx，防止重复取\n",
    "            rand_set.append(idxs_dict[label].pop(idx))\n",
    "        # print(rand_set)\n",
    "        # 将rand_set拼接成一条数据，放入dic_user[i]中\n",
    "        dict_users[i] = np.concatenate(rand_set)\n",
    "\n",
    "\n",
    "    test = []\n",
    "    for key, value in dict_users.items():\n",
    "        x = np.unique(torch.tensor(dataset.targets)[value])\n",
    "        assert(len(x)) <= shard_per_user\n",
    "        test.append(value)\n",
    "    test = np.concatenate(test)\n",
    "    assert(len(test) == len(dataset))\n",
    "    assert(len(set(list(test))) == len(dataset))\n",
    "\n",
    "    return dict_users, rand_set_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def compute_emd(targets_1, targets_2):\n",
    "    \"\"\"Calculates Earth Mover's Distance between two array-like objects (dataset labels)\"\"\"\n",
    "    total_targets = []\n",
    "    total_targets.extend(list(np.unique(targets_1)))\n",
    "    total_targets.extend(list(np.unique(targets_2)))\n",
    "\n",
    "    emd = 0\n",
    "\n",
    "    counts_1 = Counter(targets_1)\n",
    "    counts_2 = Counter(targets_2)\n",
    "\n",
    "    size_1 = len(targets_1)\n",
    "    size_2 = len(targets_2)\n",
    "\n",
    "    for t in counts_1:\n",
    "        count_2 = counts_2[t] if t in counts_2 else 0\n",
    "        emd += np.abs((counts_1[t] / size_1) - (count_2 / size_2))\n",
    "\n",
    "    for t in counts_2:\n",
    "        count_1 = counts_1[t] if t in counts_1 else 0\n",
    "        emd += np.abs((counts_2[t] / size_2) - (count_1 / size_1))\n",
    "\n",
    "    return emd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.datasets import utils, MNIST, CIFAR10\n",
    "\n",
    "tra_trans = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "val_trans = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "# len(trainset) = 50000 len(testset) = 10000\n",
    "trainset = CIFAR10(root=\"./../data\", train=True, download=True, transform=tra_trans)\n",
    "testset = CIFAR10(root=\"./../data\", train=False, download=True, transform=val_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "500\n",
      "500\n",
      "[1.5000000000000002, 1.8000000000000007, 2.1000000000000005, 1.5000000000000002, 1.8000000000000003, 1.5000000000000002, 2.1000000000000005, 1.8000000000000005, 1.8000000000000007, 1.5000000000000002, 1.8000000000000003, 1.8000000000000003, 1.5000000000000002, 1.5000000000000002, 1.8000000000000003, 1.5000000000000002, 2.1000000000000005, 1.5000000000000002, 2.1000000000000005, 1.8000000000000003, 1.8000000000000007, 1.5000000000000002, 1.8000000000000003, 1.5000000000000002, 1.5000000000000002, 2.1000000000000005, 1.8000000000000003, 1.5000000000000002, 1.8000000000000007, 1.8000000000000005, 1.8000000000000003, 1.8000000000000003, 1.8000000000000007, 2.1000000000000005, 1.5000000000000002, 1.8000000000000007, 1.5000000000000002, 1.8000000000000007, 1.5000000000000002, 2.1, 1.5000000000000002, 1.5000000000000002, 1.8000000000000003, 1.5000000000000002, 1.5000000000000002, 2.1000000000000005, 1.8000000000000003, 1.5000000000000002, 1.8000000000000005, 2.1, 1.8000000000000007, 1.8000000000000005, 1.8000000000000003, 1.8000000000000003, 1.5000000000000002, 1.5000000000000002, 2.100000000000001, 1.8000000000000007, 1.5000000000000002, 1.8000000000000007, 1.8000000000000003, 1.5000000000000002, 1.5000000000000002, 2.1000000000000005, 1.8000000000000003, 2.1000000000000005, 1.5000000000000002, 1.5000000000000002, 1.8000000000000007, 1.8000000000000003, 2.400000000000001, 2.1000000000000005, 1.5000000000000002, 2.1000000000000005, 2.1000000000000005, 1.8000000000000003, 1.5000000000000002, 2.100000000000001, 1.5000000000000002, 1.5000000000000002, 1.5000000000000002, 1.8000000000000007, 2.1000000000000005, 1.5000000000000002, 2.1000000000000005, 1.5000000000000002, 1.8000000000000003, 2.1000000000000005, 1.8000000000000003, 1.8000000000000003, 1.8000000000000003, 2.1000000000000005, 1.8000000000000003, 1.8000000000000007, 2.1, 1.5000000000000002, 1.5000000000000002, 2.1000000000000005, 2.1000000000000005, 2.1000000000000005]\n",
      "1.7700000000000002\n"
     ]
    }
   ],
   "source": [
    "d_u, rsa = client_noniid(trainset, 100, 5, seed=0)\n",
    "print(len(d_u))\n",
    "print(len(d_u[0]))\n",
    "print(len(np.unique(d_u[0])))\n",
    "\n",
    "# for key, value in d_u.items():\n",
    "#     # 抽出当前client包含的class\n",
    "#     # 有可能会给client放入同类\n",
    "#     x = np.unique(torch.tensor(trainset.targets)[value])\n",
    "#     # print(torch.tensor(trainset.targets)[value])\n",
    "#     print(\"x=\", x)\n",
    "#     # print(\"value=\", value)\n",
    "#     # break\n",
    "#     assert(len(x)) <= shard_per_user\n",
    "\n",
    "emd = []\n",
    "for ix in d_u:\n",
    "    client_targets = [trainset.targets[x] for x in d_u[ix]]\n",
    "    emd.append(compute_emd(client_targets, trainset.targets))\n",
    "print(emd)\n",
    "average_emd = np.mean([compute_emd([trainset.targets[x] for x in d_u[ix]], trainset.targets) for ix in d_u], axis=0)\n",
    "# average_emd = np.mean(emd)\n",
    "print(average_emd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "dic = {1:['a','b'], 3:['q','s'], 4:['g','u'], 5:['n','v']}\n",
    "for i in dic:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}