{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/guest', '/home/guest/FedML', '/home/guest/MyExpr', '/home/guest', '/home/guest/FedML', '/home/guest/MyExpr', '/home/guest/Fed_Expr/MyExpr', '/home/guest/miniconda/envs/fedml/lib/python37.zip', '/home/guest/miniconda/envs/fedml/lib/python3.7', '/home/guest/miniconda/envs/fedml/lib/python3.7/lib-dynload', '', '/home/guest/miniconda/envs/fedml/lib/python3.7/site-packages', '/home/guest/miniconda/envs/fedml/lib/python3.7/site-packages/IPython/extensions', '/home/guest/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "# 添加环境\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"../../MyExpr\")))\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"../../FedML\")))\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"../../\")))\n",
    "\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os.path\n",
    "from torchvision.datasets import utils, MNIST, CIFAR10\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "num_workers = 2\n",
    "\n",
    "class FEMNIST(MNIST):\n",
    "    \"\"\"\n",
    "    This dataset is derived from the Leaf repository\n",
    "    (https://github.com/TalwalkarLab/leaf) pre-processing of the Extended MNIST\n",
    "    dataset, grouping examples by writer. Details about Leaf were published in\n",
    "    \"LEAF: A Benchmark for Federated Settings\" https://arxiv.org/abs/1812.01097.\n",
    "    \"\"\"\n",
    "    resources = [\n",
    "        ('https://raw.githubusercontent.com/tao-shen/FEMNIST_pytorch/master/femnist.tar.gz',\n",
    "         '59c65cec646fc57fe92d27d83afdf0ed')]\n",
    "\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None,\n",
    "                 download=False):\n",
    "        super(MNIST, self).__init__(root, transform=transform,\n",
    "                                    target_transform=target_transform)\n",
    "        self.train = train\n",
    "\n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "        if not self._check_exists():\n",
    "            raise RuntimeError('Dataset not found.' +\n",
    "                               ' You can use download=True to download it')\n",
    "        if self.train:\n",
    "            data_file = self.training_file\n",
    "        else:\n",
    "            data_file = self.test_file\n",
    "\n",
    "        self.data, self.targets, self.users_index = torch.load(os.path.join(self.processed_folder, data_file))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.data[index], int(self.targets[index])\n",
    "        img = Image.fromarray(img.numpy(), mode='F')\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        return img, target\n",
    "\n",
    "    def download(self):\n",
    "        \"\"\"Download the FEMNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
    "        import shutil\n",
    "\n",
    "        if self._check_exists():\n",
    "            return\n",
    "\n",
    "        utils.makedir_exist_ok(self.raw_folder)\n",
    "        utils.makedir_exist_ok(self.processed_folder)\n",
    "\n",
    "        # download files\n",
    "        for url, md5 in self.resources:\n",
    "            filename = url.rpartition('/')[2]\n",
    "            utils.download_and_extract_archive(url, download_root=self.raw_folder, filename=filename, md5=md5)\n",
    "\n",
    "        # process and save as torch files\n",
    "        print('Processing...')\n",
    "        shutil.move(os.path.join(self.raw_folder, self.training_file), self.processed_folder)\n",
    "        shutil.move(os.path.join(self.raw_folder, self.test_file), self.processed_folder)\n",
    "\n",
    "\n",
    "def Dataset(args):\n",
    "    trainset, testset = None, None\n",
    "\n",
    "    if args.dataset == 'cifar10':\n",
    "        tra_trans = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "        val_trans = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "        trainset = CIFAR10(root=\"./data\", train=True, download=True, transform=tra_trans)\n",
    "        testset = CIFAR10(root=\"./data\", train=False, download=True, transform=val_trans)\n",
    "\n",
    "    if args.dataset == 'femnist' or 'mnist':\n",
    "        tra_trans = transforms.Compose([\n",
    "            transforms.Pad(2, padding_mode='edge'),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        ])\n",
    "        val_trans = transforms.Compose([\n",
    "            transforms.Pad(2, padding_mode='edge'),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        ])\n",
    "        if args.dataset == 'femnist':\n",
    "            trainset = FEMNIST(root='./data', train=True, download=True, transform=tra_trans)\n",
    "            testset = FEMNIST(root='./data', train=False, download=True, transform=val_trans)\n",
    "        if args.dataset == 'mnist':\n",
    "            trainset = MNIST(root='./data', train=True, download=True, transform=tra_trans)\n",
    "            testset = MNIST(root='./data', train=False, download=True, transform=val_trans)\n",
    "\n",
    "    return trainset, testset\n",
    "\n",
    "\n",
    "class Data(object):\n",
    "\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.trainset, self.testset = None, None\n",
    "\n",
    "        trainset, testset = Dataset(args)\n",
    "        # print(trainset, type(trainset))\n",
    "        # print(testset, type(testset))\n",
    "        # 标签\n",
    "        # print(trainset.targets, testset.targets)\n",
    "        \n",
    "        \n",
    "        # 均等分训练数据集\n",
    "        num_train = [int(len(trainset) / args.split) for _ in range(args.split)]\n",
    "        # print(\"num_train:\", num_train)\n",
    "\n",
    "        # 求前缀和\n",
    "        cumsum_train = torch.tensor(list(num_train)).cumsum(dim=0).tolist()\n",
    "        # print(\"cumsum_train:\", cumsum_train)\n",
    "        \n",
    "        # # 将trainset的下标按标签进行排序，然后放到idx_train， 下同\n",
    "        idx_train = sorted(range(len(trainset.targets)), key=lambda k: trainset.targets[k])  #split by class\n",
    "        # 将trainset的下标抽出放到idx_train，下同\n",
    "        # idx_train = range(len(trainset.targets))\n",
    "        \n",
    "        # 将划分好的下标，按照num_train划分成不同子集，【前缀和-num_train: 前缀和】\n",
    "        splited_trainset = [Subset(trainset, idx_train[off - l:off]) for off, l in zip(cumsum_train, num_train)]\n",
    "        \n",
    "        num_test = [int(len(testset) / args.split) for _ in range(args.split)]\n",
    "        cumsum_test = torch.tensor(list(num_test)).cumsum(dim=0).tolist()\n",
    "        \n",
    "        idx_test = sorted(range(len(testset.targets)), key=lambda k: testset.targets[k])  #split by class        \n",
    "        \n",
    "        # idx_test = range(len(testset.targets))\n",
    "        \n",
    "        splited_testset = [Subset(testset, idx_test[off - l:off]) for off, l in zip(cumsum_test, num_test)]\n",
    "        # print(splited_testset)\n",
    "        \n",
    "        self.train_all = DataLoader(trainset, batch_size=args.batchsize, shuffle=False, num_workers=num_workers)\n",
    "        self.test_all = DataLoader(testset, batch_size=args.batchsize, shuffle=False, num_workers=num_workers)\n",
    "        \n",
    "        self.train_loader = [DataLoader(splited_trainset[i], batch_size=args.batchsize, shuffle=True, num_workers=num_workers)\n",
    "                             for i in range(args.node_num)]\n",
    "        self.test_loader = [DataLoader(splited_testset[i], batch_size=args.batchsize, shuffle=False, num_workers=num_workers)\n",
    "                            for i in range(args.node_num)]\n",
    "        # 重复test_all>>>\n",
    "        # self.test_loader = DataLoader(testset, batch_size=args.batchsize, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--split', type=int, default=5,\n",
    "                        help='data split')\n",
    "parser.add_argument('--batchsize', type=int, default=128,\n",
    "                        help='batchsize')\n",
    "parser.add_argument('--node_num', type=int, default=5,\n",
    "                    help='Number of nodes')\n",
    "parser.add_argument('--dataset', type=str, default='cifar10',\n",
    "                    help='datasets: {cifar100, cifar10, femnist, mnist}')\n",
    "args = parser.parse_known_args()[0]\n",
    "\n",
    "data = Data(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5000, 1: 5000, 2: 5000, 3: 5000, 4: 5000, 5: 5000, 6: 5000, 7: 5000, 8: 5000, 9: 5000}\n"
     ]
    }
   ],
   "source": [
    "# 总体数据统计\n",
    "keys = [i for i in range(10)]\n",
    "# {0: 5000, 1: 5000, 2: 5000, 3: 5000, 4: 5000, 5: 5000, 6: 5000, 7: 5000, 8: 5000, 9: 5000}\n",
    "ts = data.train_all\n",
    "#{0: 1000, 1: 1000, 2: 1000, 3: 1000, 4: 1000, 5: 1000, 6: 1000, 7: 1000, 8: 1000, 9: 1000}\n",
    "# ts = data.test_all\n",
    "dic = {}\n",
    "for idx, (x, y) in enumerate(ts):\n",
    "    for key in keys:\n",
    "        key_sum = (y == key).sum().item()\n",
    "        if key in dic:\n",
    "            dic[key] += key_sum\n",
    "        else:\n",
    "            dic[key] = key_sum\n",
    "print(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client 0 : {0: 5000, 1: 5000, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0}\n",
      "client 1 : {0: 0, 1: 0, 2: 5000, 3: 5000, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0}\n",
      "client 2 : {0: 0, 1: 0, 2: 0, 3: 0, 4: 5000, 5: 5000, 6: 0, 7: 0, 8: 0, 9: 0}\n",
      "client 3 : {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 5000, 7: 5000, 8: 0, 9: 0}\n",
      "client 4 : {0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0, 7: 0, 8: 5000, 9: 5000}\n"
     ]
    }
   ],
   "source": [
    "# 数据划分统计\n",
    "ts = data.train_loader\n",
    "for i in range(args.node_num):\n",
    "    dic = {}\n",
    "    for idx, (x, y) in enumerate(ts[i]):\n",
    "        for key in keys:\n",
    "            key_sum = (y == key).sum().item()\n",
    "            # print(key, key_sum)\n",
    "        # break\n",
    "            if key in dic:\n",
    "                dic[key] += key_sum\n",
    "            else:\n",
    "                dic[key] = key_sum\n",
    "    print(\"client\", i, \":\", dic)\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "s = torch.rand(1)\n",
    "print(s)\n",
    "print(s.item())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7148])\n",
      "0.7147847414016724\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}