{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/ubuntu/Fed_Expr', '/home/ubuntu/Fed_Expr/FedML', '/home/ubuntu/Fed_Expr/MyExpr', '/home/ubuntu/Fed_Expr/MyExpr/ml', '/home/ubuntu/miniconda/envs/fedml/lib/python37.zip', '/home/ubuntu/miniconda/envs/fedml/lib/python3.7', '/home/ubuntu/miniconda/envs/fedml/lib/python3.7/lib-dynload', '', '/home/ubuntu/miniconda/envs/fedml/lib/python3.7/site-packages', '/home/ubuntu/miniconda/envs/fedml/lib/python3.7/site-packages/IPython/extensions', '/home/ubuntu/.ipython']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# 添加环境\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"../../MyExpr\")))\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"../../FedML\")))\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), \"../../\")))\n",
    "\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# logistic\n",
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = torch.sigmoid(self.linear(x))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# todo neural network\n",
    "# 5优化神经网络\n",
    "# 5.1模型改进\n",
    "class Activation_Net(nn.Module):\n",
    "    def __init__(self, in_dim, n_hidden_1, n_hidden_2,n_hidden_3, out_dim):\n",
    "        super(Activation_Net, self).__init__()\n",
    "        self.layer1 = nn.Sequential(nn.Linear(in_dim, n_hidden_1), nn.ReLU(True))\n",
    "        self.layer2 = nn.Sequential(nn.Linear(n_hidden_1, n_hidden_2), nn.ReLU(True))\n",
    "        self.layer3 = nn.Sequential(nn.Linear(n_hidden_2, n_hidden_3), nn.ReLU(True))\n",
    "        \"\"\"\n",
    "        请补充 利用nn.Sequential填写最后一层代码\n",
    "        \"\"\"\n",
    "        self.layer4 = nn.Linear(n_hidden_3, out_dim)\n",
    "        # nn.Sequential(nn.Linear(n_hidden_3, out_dim), nn.Softmax())\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        return x\n",
    "\n",
    "# 检查模型是否正确构建\n",
    "model = Activation_Net(18, 100, 50, 30, 1)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda/envs/fedml/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/ubuntu/miniconda/envs/fedml/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/ubuntu/miniconda/envs/fedml/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/home/ubuntu/miniconda/envs/fedml/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: greenlet.greenlet size changed, may indicate binary incompatibility. Expected 144 from C header, got 152 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mkyriegyj\u001B[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/kyriegyj/susy_nonfl/runs/1crz2yvy\" target=\"_blank\">still-serenity-18</a></strong> to <a href=\"https://wandb.ai/kyriegyj/susy_nonfl\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc9d06b9df0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(\"cifar\")\n",
    "parser.add_argument('--data_name', type=str, default=\"SUSY\", help='SUSY; RO')\n",
    "parser.add_argument('--epoch_size', type=int, default=500, help='1,2,3,4,5')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.1)\n",
    "parser.add_argument(\"--batch_size\", type=int, default=1000)\n",
    "parser.add_argument('--weight_decay', type=float, default=0.0001)\n",
    "\n",
    "# 报错，因为Jupyter会读入一个默认的环境参数-f\n",
    "# parser.parse_args()\n",
    "args = parser.parse_known_args()[0]\n",
    "\n",
    "# wandb\n",
    "import wandb\n",
    "\n",
    "wandb.init(project=\"susy_nonfl\",\n",
    "            entity=\"kyriegyj\",\n",
    "           config=args)\n",
    "\n",
    "# fix random seeds\n",
    "seed = 127834698\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "from MyExpr.ml.data_loader_for_susy import DataLoader\n",
    "\n",
    "epoch_size = args.epoch_size\n",
    "batch_size = args.batch_size\n",
    "\n",
    "reload = False\n",
    "\n",
    "data_loader = DataLoader(\"../../FedML/data/UCI/SUSY/SUSY.csv\", batch_size = batch_size)\n",
    "\n",
    "if reload:\n",
    "    train_X, train_Y, test_X, test_Y = data_loader.load_data()\n",
    "# print(len(train_X), len(test_X), batch_size, \"5,000,000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4500 1000\n",
      "train_loss: 2132.747630894184 acc: 0.7832413333333333 epoch: 0\n",
      "train_loss: 2079.321749597788 acc: 0.78822 epoch: 1\n",
      "train_loss: 2074.062014937401 acc: 0.7884904444444445 epoch: 2\n",
      "train_loss: 2071.4913159012794 acc: 0.7885322222222222 epoch: 3\n",
      "train_loss: 2069.858170300722 acc: 0.7885693333333333 epoch: 4\n",
      "train_loss: 2069.058108061552 acc: 0.7885251111111111 epoch: 5\n",
      "train_loss: 2068.5616105794907 acc: 0.7884724444444444 epoch: 6\n",
      "train_loss: 2068.3146290779114 acc: 0.7884655555555555 epoch: 7\n",
      "train_loss: 2068.159271001816 acc: 0.788436 epoch: 8\n",
      "train_loss: 2068.0593761503696 acc: 0.7884353333333334 epoch: 9\n",
      "train_loss: 2067.992193311453 acc: 0.7884317777777777 epoch: 10\n",
      "train_loss: 2067.9471566081047 acc: 0.7884268888888889 epoch: 11\n",
      "train_loss: 2067.9161597788334 acc: 0.7884304444444444 epoch: 12\n",
      "train_loss: 2067.8941952586174 acc: 0.7884302222222223 epoch: 13\n",
      "train_loss: 2067.8784460127354 acc: 0.7884302222222223 epoch: 14\n",
      "train_loss: 2067.7831463217735 acc: 0.788428 epoch: 15\n",
      "train_loss: 2067.774670571089 acc: 0.7884304444444444 epoch: 16\n",
      "train_loss: 2067.768277347088 acc: 0.7884293333333333 epoch: 17\n",
      "train_loss: 2067.7635293900967 acc: 0.7884326666666667 epoch: 18\n",
      "train_loss: 2067.7598444521427 acc: 0.7884322222222222 epoch: 19\n",
      "train_loss: 2067.7566832304 acc: 0.7884337777777778 epoch: 20\n",
      "train_loss: 2067.75419947505 acc: 0.7884357777777777 epoch: 21\n",
      "train_loss: 2067.7521253228188 acc: 0.788434 epoch: 22\n",
      "train_loss: 2067.7503331005573 acc: 0.7884337777777778 epoch: 23\n",
      "train_loss: 2067.748751372099 acc: 0.7884344444444444 epoch: 24\n",
      "train_loss: 2067.7473990619183 acc: 0.788432 epoch: 25\n",
      "train_loss: 2067.746260225773 acc: 0.7884313333333334 epoch: 26\n",
      "train_loss: 2067.7452116906643 acc: 0.7884313333333334 epoch: 27\n",
      "train_loss: 2067.7443391680717 acc: 0.7884311111111111 epoch: 28\n",
      "train_loss: 2067.743474394083 acc: 0.7884317777777777 epoch: 29\n",
      "train_loss: 2067.742698073387 acc: 0.7884304444444444 epoch: 30\n",
      "train_loss: 2067.7419788241386 acc: 0.7884291111111111 epoch: 31\n",
      "train_loss: 2067.741397023201 acc: 0.7884304444444444 epoch: 32\n",
      "train_loss: 2067.740798354149 acc: 0.7884315555555556 epoch: 33\n",
      "train_loss: 2067.7395919561386 acc: 0.7884317777777777 epoch: 34\n",
      "train_loss: 2067.7391007840633 acc: 0.7884311111111111 epoch: 35\n",
      "train_loss: 2067.822404205799 acc: 0.7884322222222222 epoch: 36\n",
      "train_loss: 2067.8219914138317 acc: 0.7884331111111111 epoch: 37\n",
      "train_loss: 2067.821621596813 acc: 0.788432 epoch: 38\n",
      "train_loss: 2067.821275562048 acc: 0.7884335555555556 epoch: 39\n",
      "train_loss: 2067.820993512869 acc: 0.7884348888888889 epoch: 40\n",
      "train_loss: 2067.8206844329834 acc: 0.788436 epoch: 41\n",
      "train_loss: 2067.82044968009 acc: 0.7884355555555556 epoch: 42\n",
      "train_loss: 2067.8201885819435 acc: 0.7884364444444445 epoch: 43\n",
      "train_loss: 2067.8199934363365 acc: 0.788436 epoch: 44\n",
      "train_loss: 2067.819799631834 acc: 0.7884357777777777 epoch: 45\n",
      "train_loss: 2067.819599598646 acc: 0.7884353333333334 epoch: 46\n",
      "train_loss: 2067.8195095062256 acc: 0.788436 epoch: 47\n",
      "train_loss: 2067.819342672825 acc: 0.788436 epoch: 48\n",
      "train_loss: 2067.819191366434 acc: 0.7884364444444445 epoch: 49\n",
      "train_loss: 2067.8190391659737 acc: 0.7884357777777777 epoch: 50\n",
      "train_loss: 2067.8189087212086 acc: 0.7884351111111111 epoch: 51\n",
      "train_loss: 2067.8187891840935 acc: 0.7884362222222222 epoch: 52\n",
      "train_loss: 2067.81867736578 acc: 0.7884368888888889 epoch: 53\n",
      "train_loss: 2067.818577259779 acc: 0.7884368888888889 epoch: 54\n",
      "train_loss: 2067.818478643894 acc: 0.7884373333333333 epoch: 55\n",
      "train_loss: 2067.818395525217 acc: 0.788438 epoch: 56\n",
      "train_loss: 2067.8183108866215 acc: 0.788438 epoch: 57\n",
      "train_loss: 2067.818239569664 acc: 0.7884382222222223 epoch: 58\n",
      "train_loss: 2067.8182187974453 acc: 0.7884382222222223 epoch: 59\n",
      "train_loss: 2067.8181685507298 acc: 0.7884382222222223 epoch: 60\n",
      "train_loss: 2067.8181414306164 acc: 0.7884377777777778 epoch: 61\n",
      "train_loss: 2067.8180949389935 acc: 0.7884382222222223 epoch: 62\n",
      "train_loss: 2067.818064004183 acc: 0.7884384444444444 epoch: 63\n",
      "train_loss: 2067.8180237412453 acc: 0.7884382222222223 epoch: 64\n",
      "train_loss: 2067.817992180586 acc: 0.7884382222222223 epoch: 65\n",
      "train_loss: 2067.817952990532 acc: 0.7884382222222223 epoch: 66\n",
      "train_loss: 2067.817920356989 acc: 0.788438 epoch: 67\n",
      "train_loss: 2067.8178889751434 acc: 0.7884382222222223 epoch: 68\n",
      "train_loss: 2067.817855834961 acc: 0.7884384444444444 epoch: 69\n",
      "train_loss: 2067.8178857266903 acc: 0.7884384444444444 epoch: 70\n",
      "train_loss: 2067.8178553283215 acc: 0.7884386666666666 epoch: 71\n",
      "train_loss: 2067.8178329765797 acc: 0.7884386666666666 epoch: 72\n",
      "train_loss: 2067.817812770605 acc: 0.7884386666666666 epoch: 73\n",
      "train_loss: 2067.817790746689 acc: 0.7884384444444444 epoch: 74\n",
      "train_loss: 2067.817778378725 acc: 0.7884384444444444 epoch: 75\n",
      "train_loss: 2067.8177694380283 acc: 0.7884391111111111 epoch: 76\n",
      "train_loss: 2067.8177499473095 acc: 0.7884391111111111 epoch: 77\n",
      "train_loss: 2067.817740082741 acc: 0.7884391111111111 epoch: 78\n",
      "train_loss: 2067.8177277445793 acc: 0.7884391111111111 epoch: 79\n",
      "train_loss: 2067.817715138197 acc: 0.7884393333333334 epoch: 80\n",
      "train_loss: 2067.8177050054073 acc: 0.7884391111111111 epoch: 81\n",
      "train_loss: 2067.817690640688 acc: 0.7884393333333334 epoch: 82\n",
      "train_loss: 2067.8176808953285 acc: 0.7884393333333334 epoch: 83\n",
      "train_loss: 2067.8176731169224 acc: 0.7884388888888889 epoch: 84\n",
      "train_loss: 2067.8176619410515 acc: 0.7884395555555556 epoch: 85\n",
      "train_loss: 2067.8176540732384 acc: 0.7884395555555556 epoch: 86\n",
      "train_loss: 2067.81764793396 acc: 0.7884395555555556 epoch: 87\n",
      "train_loss: 2067.8176421523094 acc: 0.7884395555555556 epoch: 88\n",
      "train_loss: 2067.817636370659 acc: 0.7884393333333334 epoch: 89\n",
      "train_loss: 2067.8176314532757 acc: 0.7884393333333334 epoch: 90\n",
      "train_loss: 2067.817623347044 acc: 0.7884393333333334 epoch: 91\n",
      "train_loss: 2067.8176198601723 acc: 0.7884393333333334 epoch: 92\n",
      "train_loss: 2067.8176179230213 acc: 0.7884393333333334 epoch: 93\n",
      "train_loss: 2067.8176098167896 acc: 0.7884393333333334 epoch: 94\n",
      "train_loss: 2067.817608386278 acc: 0.7884393333333334 epoch: 95\n",
      "train_loss: 2067.8175984323025 acc: 0.7884393333333334 epoch: 96\n",
      "train_loss: 2067.817595809698 acc: 0.7884393333333334 epoch: 97\n",
      "train_loss: 2067.8175930678844 acc: 0.7884393333333334 epoch: 98\n",
      "train_loss: 2067.817589879036 acc: 0.7884393333333334 epoch: 99\n",
      "train_loss: 2067.8175875246525 acc: 0.7884393333333334 epoch: 100\n",
      "train_loss: 2067.8175846636295 acc: 0.7884393333333334 epoch: 101\n",
      "train_loss: 2067.817582935095 acc: 0.7884393333333334 epoch: 102\n",
      "train_loss: 2067.817581117153 acc: 0.7884393333333334 epoch: 103\n",
      "train_loss: 2067.8175804913044 acc: 0.7884393333333334 epoch: 104\n",
      "train_loss: 2067.8175791502 acc: 0.7884393333333334 epoch: 105\n",
      "train_loss: 2067.8175787329674 acc: 0.7884393333333334 epoch: 106\n",
      "train_loss: 2067.817576766014 acc: 0.7884393333333334 epoch: 107\n",
      "train_loss: 2067.817575842142 acc: 0.7884393333333334 epoch: 108\n",
      "train_loss: 2067.817572116852 acc: 0.7884393333333334 epoch: 109\n",
      "train_loss: 2067.8175706267357 acc: 0.7884393333333334 epoch: 110\n",
      "train_loss: 2067.8175733089447 acc: 0.7884393333333334 epoch: 111\n",
      "train_loss: 2067.8175694942474 acc: 0.7884393333333334 epoch: 112\n",
      "train_loss: 2067.817570656538 acc: 0.7884393333333334 epoch: 113\n",
      "train_loss: 2067.817568540573 acc: 0.7884393333333334 epoch: 114\n",
      "train_loss: 2067.817566305399 acc: 0.7884393333333334 epoch: 115\n",
      "train_loss: 2067.81756734848 acc: 0.7884393333333334 epoch: 116\n",
      "train_loss: 2067.8175686597824 acc: 0.7884393333333334 epoch: 117\n",
      "train_loss: 2067.8175677359104 acc: 0.7884393333333334 epoch: 118\n",
      "train_loss: 2067.81756567955 acc: 0.7884393333333334 epoch: 119\n",
      "train_loss: 2067.817562520504 acc: 0.7884393333333334 epoch: 120\n",
      "train_loss: 2067.817563354969 acc: 0.7884393333333334 epoch: 121\n",
      "train_loss: 2067.8175621032715 acc: 0.7884393333333334 epoch: 122\n",
      "train_loss: 2067.8175631463528 acc: 0.7884393333333334 epoch: 123\n",
      "train_loss: 2067.817561209202 acc: 0.7884393333333334 epoch: 124\n",
      "train_loss: 2067.817561507225 acc: 0.7884393333333334 epoch: 125\n",
      "train_loss: 2067.8175623714924 acc: 0.7884393333333334 epoch: 126\n",
      "train_loss: 2067.817561417818 acc: 0.7884393333333334 epoch: 127\n",
      "train_loss: 2067.817562073469 acc: 0.7884393333333334 epoch: 128\n",
      "train_loss: 2067.817562609911 acc: 0.7884393333333334 epoch: 129\n",
      "train_loss: 2067.8175616562366 acc: 0.7884393333333334 epoch: 130\n",
      "train_loss: 2067.8175625801086 acc: 0.7884393333333334 epoch: 131\n",
      "train_loss: 2067.8175633847713 acc: 0.7884393333333334 epoch: 132\n",
      "train_loss: 2067.81756234169 acc: 0.7884393333333334 epoch: 133\n",
      "train_loss: 2067.817560404539 acc: 0.7884393333333334 epoch: 134\n",
      "train_loss: 2067.817562997341 acc: 0.7884393333333334 epoch: 135\n",
      "train_loss: 2067.817561507225 acc: 0.7884393333333334 epoch: 136\n",
      "train_loss: 2067.817562907934 acc: 0.7884393333333334 epoch: 137\n",
      "train_loss: 2067.8175616264343 acc: 0.7884393333333334 epoch: 138\n",
      "train_loss: 2067.817562133074 acc: 0.7884393333333334 epoch: 139\n",
      "train_loss: 2067.8175578713417 acc: 0.7884393333333334 epoch: 140\n",
      "train_loss: 2067.81756106019 acc: 0.7884393333333334 epoch: 141\n",
      "train_loss: 2067.81755939126 acc: 0.7884393333333334 epoch: 142\n",
      "train_loss: 2067.8175606429577 acc: 0.7884393333333334 epoch: 143\n",
      "train_loss: 2067.8175598978996 acc: 0.7884393333333334 epoch: 144\n",
      "train_loss: 2067.817560404539 acc: 0.7884393333333334 epoch: 145\n",
      "train_loss: 2067.817559659481 acc: 0.7884393333333334 epoch: 146\n",
      "train_loss: 2067.817560136318 acc: 0.7884393333333334 epoch: 147\n",
      "train_loss: 2067.8175594210625 acc: 0.7884393333333334 epoch: 148\n",
      "train_loss: 2067.8175591528416 acc: 0.7884393333333334 epoch: 149\n",
      "train_loss: 2067.817557632923 acc: 0.7884393333333334 epoch: 150\n",
      "train_loss: 2067.8175599575043 acc: 0.7884393333333334 epoch: 151\n",
      "train_loss: 2067.8175581991673 acc: 0.7884393333333334 epoch: 152\n",
      "train_loss: 2067.8175553679466 acc: 0.7884393333333334 epoch: 153\n",
      "train_loss: 2067.817558825016 acc: 0.7884393333333334 epoch: 154\n",
      "train_loss: 2067.817560583353 acc: 0.7884393333333334 epoch: 155\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "learning_rate = args.learning_rate\n",
    "weight_decay = args.weight_decay\n",
    "\n",
    "model = LogisticRegression(18, 1)\n",
    "iteration = len(train_X)\n",
    "# print(iteration, batch_size)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "# print(optimizer)\n",
    "# print(model)\n",
    "\n",
    "test = False\n",
    "for e in range(epoch_size):\n",
    "    # train\n",
    "    correct = 0\n",
    "    total_loss = 0\n",
    "    for i in range(iteration):\n",
    "        x, y = torch.tensor(np.asarray(train_X[i], dtype=np.float32), dtype=torch.float32), torch.tensor(np.asarray(train_Y[i], dtype=np.float32), dtype=torch.float32)\n",
    "       \n",
    "        # print(x)\n",
    "        # mean_x = torch.mean(x, dim = 1)\n",
    "        # std_x = x.std(dim = 1)\n",
    "        # # print(mean_x.shape, std_x.shape)\n",
    "        # x = (x - mean_x.unsqueeze(1).expand(x.shape[0], x.shape[1])) / std_x.unsqueeze(1).expand(x.shape[0], x.shape[1])\n",
    "        # print(x)\n",
    "        \n",
    "        # [batch_size, 1]\n",
    "        output = model(x)\n",
    "        # print(output)\n",
    "        # print(x, y)\n",
    "        # print(type(x), type(y))\n",
    "        # print(x.shape, y.shape, output.shape)\n",
    "        \n",
    "        # 1 dimension\n",
    "        loss = criterion(output.squeeze(1), y)\n",
    "        \n",
    "        # 求第一维的最大值以及对应下标，应该是配合softmax使用(取预测概率最大的类下标作为预测结果，但是二分类只有一个输出。。)\n",
    "        # pred[0]为最大值，pred[1]为最大值的下标\n",
    "        # pred = output.max(1, keepdim = True)[1]\n",
    "        \n",
    "        # 求第一维的最大值对应的下标\n",
    "        # pred = torch.argmax(output, 1);\n",
    "        pred = output.ge(0.5).float()\n",
    "        # print(pred)\n",
    "        # print(pred.shape, y.shape)\n",
    "        # print(output.shape, loss, pred[0].shape, pred[1].shape, len(pred))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        correct += (pred.squeeze(1) == y).sum().item()\n",
    "        total_loss += loss.detach().numpy()\n",
    "        # print(correct, pred.shape)\n",
    "        if test: break\n",
    "    if test: break\n",
    "    \n",
    "    print(\"train_loss:\", total_loss, \"train_acc:\", float(correct / (iteration * batch_size)), \"epoch:\", e)\n",
    "    if not test:\n",
    "        wandb.log({\"train_loss\": total_loss, \"train_acc\": float(correct / (iteration * batch_size)), \"epoch\": e})\n",
    "    \n",
    "\n",
    "if not test:\n",
    "    log_file_path = \"log/wandb.log\"\n",
    "    wandb.save(log_file_path)\n",
    "    # if test:\n",
    "    #     break\n",
    "       \n",
    "        # output =\n",
    "    # test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_acc = 0\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_X)):\n",
    "        x, y = torch.tensor(np.asarray(test_X[i], dtype=np.float32), dtype=torch.float32), torch.tensor(np.asarray(test_Y[i], dtype=np.float32), dtype=torch.float32)\n",
    "        output = model(x)\n",
    "        loss = criterion(output.squeeze(1), y)\n",
    "        pred = output.ge(0.5).float()\n",
    "        correct += (pred.squeeze(1) == y).sum().item()\n",
    "        total_loss += loss.detach().numpy()\n",
    "    print(\"test_loss:\", total_loss, \"test_acc:\", float(correct / (len(test_X) * batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# a = torch.tensor(np.asarray([[1.1], [0.7], [0.9]], dtype=np.float32), dtype=torch.float32)\n",
    "# print(a)\n",
    "# print(a.max(1, keepdim = True))\n",
    "\n",
    "# a = torch.rand(3, 1)\n",
    "# print(a)\n",
    "# print(a.max(1, keepdim = True))\n",
    "\n",
    "# a = torch.randn(10, 2)\n",
    "# b = torch.randn(10)\n",
    "\n",
    "# # print(a.shape)\n",
    "# # print(b.shape)\n",
    "# print(b)\n",
    "# print(b.unsqueeze(1).expand(10,2))\n",
    "\n",
    "a = torch.tensor(np.asarray([[0.1], [0.7], [0.9]], dtype=np.float32), dtype=torch.float32)\n",
    "print(a.shape)\n",
    "print(a[a>0.5].shape[0])\n",
    "print(a.ge(0.5).float())\n",
    "\n",
    "print(torch.tensor([True, False]).sum())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}